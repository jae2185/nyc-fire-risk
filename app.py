"""
ğŸ”¥ NYC Fire Risk Prediction Dashboard

Multi-resolution fire risk prediction for New York City using
RandomForest on public FDNY incident data. Supports zip code,
PUMA, and borough granularity.
"""

import streamlit as st
import pandas as pd
import numpy as np
import plotly.graph_objects as go

from data.fetch_data import FireDataPipeline
from data.feature_engineering import (
    engineer_features_by_zip,
    aggregate_to_puma,
    aggregate_to_borough,
    get_feature_matrix,
)
from models.fire_model import FireRiskModel
from utils.visualization import (
    COLORS, risk_color, risk_label,
    make_feature_importance_chart,
    make_actual_vs_predicted_chart,
    make_risk_distribution_chart,
    make_borough_comparison_chart,
    make_monthly_chart,
    create_folium_map,
)
from data.building_data import (
    fetch_pluto_buildings,
    process_pluto_buildings,
    score_buildings_with_neighborhood,
)
from data.enrichment import enrich_zip_features
from models.enhanced_model import (
    get_enhanced_feature_matrix,
    compare_models,
    train_tuned_rf,
    train_xgboost,
)

# â”€â”€â”€ Page Config â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.set_page_config(
    page_title="NYC Fire Risk Prediction",
    page_icon="ğŸ”¥",
    layout="wide",
    initial_sidebar_state="expanded",
)

# â”€â”€â”€ Session State â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if "selected_zone" not in st.session_state:
    st.session_state.selected_zone = None

# â”€â”€â”€ Custom CSS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.markdown("""
<style>
    @import url('https://fonts.googleapis.com/css2?family=JetBrains+Mono:wght@400;500;700;800&display=swap');

    .stApp { font-family: 'JetBrains Mono', monospace; }
    .risk-critical { background: #FF3B4E20; color: #FF3B4E; padding: 3px 12px; border-radius: 12px; font-size: 11px; font-weight: 600; border: 1px solid #FF3B4E40; }
    .risk-high { background: #FFAA2B20; color: #FFAA2B; padding: 3px 12px; border-radius: 12px; font-size: 11px; font-weight: 600; border: 1px solid #FFAA2B40; }
    .risk-moderate { background: #FF6B3520; color: #FF6B35; padding: 3px 12px; border-radius: 12px; font-size: 11px; font-weight: 600; border: 1px solid #FF6B3540; }
    .risk-low { background: #2ECC7120; color: #2ECC71; padding: 3px 12px; border-radius: 12px; font-size: 11px; font-weight: 600; border: 1px solid #2ECC7140; }
    div[data-testid="stSidebar"] { background: #0D1117; }
</style>
""", unsafe_allow_html=True)


# â”€â”€â”€ Data Loading & Model Training (with disk cache) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import pickle
import os
from pathlib import Path

CACHE_DIR = Path(__file__).parent / ".cache"
CACHE_FILE = CACHE_DIR / "model_cache.pkl"
CACHE_MAX_AGE_HOURS = 24  # Retrain after this many hours


def _cache_is_fresh():
    """Check if the disk cache exists and is recent enough."""
    slim_cache = Path(__file__).parent / "data" / "model_cache_slim.pkl"
    if slim_cache.exists() and not CACHE_FILE.exists():
        return True  # Slim cache is always fresh (pre-built for deployment)
    if not CACHE_FILE.exists():
        return False
    import time
    age_hours = (time.time() - CACHE_FILE.stat().st_mtime) / 3600
    return age_hours < CACHE_MAX_AGE_HOURS


def _save_cache(data):
    """Save model results to disk."""
    CACHE_DIR.mkdir(exist_ok=True)
    with open(CACHE_FILE, "wb") as f:
        pickle.dump(data, f)


def _load_cache():
    """Load model results from disk. Tries slim cache first (for cloud deployment)."""
    slim_cache = Path(__file__).parent / "data" / "model_cache_slim.pkl"
    if slim_cache.exists() and not CACHE_FILE.exists():
        with open(slim_cache, "rb") as f:
            return pickle.load(f)
    with open(CACHE_FILE, "rb") as f:
        return pickle.load(f)


@st.cache_resource(show_spinner=False)
def load_and_train(force_retrain=False):
    """
    Full pipeline: fetch data, engineer features, train model.

    Results are cached to disk so the model doesn't retrain on every
    app restart. Cache expires after CACHE_MAX_AGE_HOURS hours.
    Pass force_retrain=True to ignore the cache.
    """
    # Try loading from disk cache first
    if not force_retrain and _cache_is_fresh():
        try:
            cached = _load_cache()
            # Verify it has the expected keys
            if all(k in cached for k in ["zip_features", "model", "results"]):
                return cached
        except Exception:
            pass  # Cache corrupted, retrain

    # Full pipeline
    pipeline = FireDataPipeline()
    df = pipeline.fetch_and_process(limit=50000)

    zip_features = engineer_features_by_zip(df)

    # Enrich with external data (DOB, 311, PLUTO)
    try:
        zip_features = enrich_zip_features(zip_features)
        print(f"[PIPELINE] Enriched features: {len(zip_features.columns)} columns")
    except Exception as e:
        print(f"[PIPELINE] Enrichment failed (continuing with base features): {e}")

    puma_features = aggregate_to_puma(zip_features)
    boro_features = aggregate_to_borough(zip_features)

    # Train enhanced model (tries both RF and XGBoost)
    X, y, feature_names = get_enhanced_feature_matrix(zip_features)
    if X.shape[1] == 0:
        # Fallback to original features if enhanced matrix is empty
        from data.feature_engineering import get_feature_matrix as get_orig_features
        X, y, feature_names = get_orig_features(zip_features)

    model_comparison = compare_models(X, y, feature_names)
    best = model_comparison["best"]

    # Use the best model
    model = FireRiskModel.__new__(FireRiskModel)
    model.model = best["model"]
    model.feature_names = feature_names
    model.importance = best["importance"]
    results = best

    # Zip predictions
    zip_preds = best["model"].predict(X)
    zip_risk = np.clip(zip_preds / (zip_preds.max() or 1), 0, 1)
    zip_features["predicted_fires"] = zip_preds
    zip_features["risk_score"] = zip_risk
    zip_features["risk_label"] = zip_features["risk_score"].map(risk_label)

    # PUMA-level prediction
    from data.feature_engineering import get_feature_matrix as get_orig_features
    X_puma, y_puma, fn_puma = get_orig_features(puma_features)
    if len(X_puma) > 5:
        from sklearn.ensemble import RandomForestRegressor
        puma_rf = RandomForestRegressor(n_estimators=80, max_depth=8, random_state=42)
        puma_rf.fit(X_puma, y_puma)
        puma_preds = puma_rf.predict(X_puma)
        puma_risk = np.clip(puma_preds / (puma_preds.max() or 1), 0, 1)
    else:
        puma_preds = y_puma
        puma_risk = np.clip(y_puma / (y_puma.max() or 1), 0, 1)
    puma_features["predicted_fires"] = puma_preds
    puma_features["risk_score"] = puma_risk
    puma_features["risk_label"] = puma_features["risk_score"].map(risk_label)

    # Borough-level
    max_sf = boro_features["structural_fires"].max() or 1
    boro_features["risk_score"] = np.clip(boro_features["structural_fires"] / max_sf, 0, 1)
    boro_features["risk_label"] = boro_features["risk_score"].map(risk_label)
    boro_features["predicted_fires"] = boro_features["structural_fires"]

    data = {
        "raw_df": df,
        "zip_features": zip_features,
        "puma_features": puma_features,
        "boro_features": boro_features,
        "model": model,
        "results": results,
        "X": X, "y": y,
        "zip_risk": zip_risk,
    }

    # Save to disk
    _save_cache(data)

    return data


# â”€â”€â”€ Main App â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def main():
    # Sidebar
    with st.sidebar:
        st.markdown("## ğŸ”¥ NYC Fire Risk")
        st.caption("PREDICTIVE INTELLIGENCE")
        st.divider()

        granularity = st.radio(
            "**Spatial Granularity**",
            ["Zip Code", "PUMA", "Borough"],
            help="Zip code (~180 zones), PUMA/neighborhood (~55), Borough (5)",
        )

        st.divider()
        st.markdown(
            '<div style="font-size:10px;color:#7B8DA4;line-height:1.6">'
            '<b style="color:#FF6B35">DATA</b><br>'
            'FDNY NYFIRS via NYC Open Data<br>'
            'SODA endpoint: tm6d-hbzd<br><br>'
            '<b style="color:#FF6B35">MODEL</b><br>'
            'RandomForest (n=100, depth=12)<br>'
            'scikit-learn + SHAP</div>',
            unsafe_allow_html=True,
        )

        st.divider()
        if st.button("ğŸ”„ Retrain Model", width="stretch",
                      help="Force re-fetch data and retrain. Use if data is stale."):
            if CACHE_FILE.exists():
                CACHE_FILE.unlink()
            st.cache_resource.clear()
            st.rerun()

    # Load data
    with st.spinner("Fetching FDNY data & training model..."):
        data = load_and_train()

    zip_features = data["zip_features"]
    puma_features = data["puma_features"]
    boro_features = data["boro_features"]
    model = data["model"]
    results = data["results"]

    # Active dataset
    if granularity == "Zip Code":
        active_df = zip_features.copy()
        label_col = "zip_code"
    elif granularity == "PUMA":
        active_df = puma_features.copy()
        label_col = "puma_name"
    else:
        active_df = boro_features.copy()
        label_col = "borough"

    # â”€â”€â”€ Header â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    st.markdown(f"## NYC Fire Risk â€” {granularity} Level")
    model_name = results.get("model_name", "RandomForest")
    n_feats = results["train"].get("n_features", "?")
    st.caption(
        f"{len(active_df)} zones Â· {model_name} Â· "
        f"{n_feats} features Â· {results['train']['n_samples']} training samples"
    )

    c1, c2, c3, c4 = st.columns(4)
    c1.metric("Zones Analyzed", len(active_df), help="Number of geographic zones included in the analysis at the selected spatial granularity.")
    c2.metric("Structural Fires", f"{int(active_df['structural_fires'].sum()):,}", help="Total structural fire incidents across all zones in the training period (2019-2022). Source: FDNY NYFIRS via NYC Open Data.")
    c3.metric("Critical Zones", len(active_df[active_df["risk_score"] >= 0.75]), help="Zones with a model-predicted risk score >= 0.75 (top quartile). These areas have the highest concentration of fire risk factors.")
    c4.metric("Model RÂ²", f"{results['train']['r2']:.3f}", help="R-squared on training data. Measures how well the model explains variance in fire counts. 1.0 = perfect fit.")

    # â”€â”€â”€ Tabs â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    tab_map, tab_rankings, tab_model, tab_explorer, tab_buildings, tab_validation = st.tabs(
        ["ğŸ—ºï¸ Risk Map", "ğŸ“Š Rankings", "ğŸ§  Model", "ğŸ” Explorer", "ğŸ¢ Buildings", "âœ… Validation"]
    )

    # â”€â”€ TAB: Map â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    with tab_map:
        col_map, col_side = st.columns([3, 1])

        # Determine map center/zoom from selected zone
        selected = st.session_state.selected_zone
        map_center = None
        map_zoom = 11 if granularity == "Zip Code" else (11 if granularity == "PUMA" else 10)

        if selected is not None:
            sel_row = active_df[active_df[label_col] == selected]
            if not sel_row.empty:
                lat = sel_row.iloc[0].get("latitude")
                lng = sel_row.iloc[0].get("longitude")
                if pd.notna(lat) and pd.notna(lng):
                    map_center = [lat, lng]
                    map_zoom = 14 if granularity == "Zip Code" else (13 if granularity == "PUMA" else 11)

        with col_map:
            try:
                from streamlit_folium import st_folium
                fmap = create_folium_map(
                    active_df.reset_index(drop=True),
                    active_df["risk_score"].values,
                    label_col,
                    center=map_center,
                    zoom=map_zoom,
                    highlight_zone=selected,
                )
                st_folium(fmap, width=None, height=520, returned_objects=[])
            except ImportError:
                st.info("Install `streamlit-folium` for interactive maps. Showing table view instead.")
                st.dataframe(
                    active_df[[label_col, "structural_fires", "risk_score", "risk_label"]]
                    .sort_values("risk_score", ascending=False),
                    width="stretch",
                )

        with col_side:
            st.markdown("**Highest Risk** *(click to focus)*")
            top_zones = active_df.nlargest(8, "risk_score")
            for _, row in top_zones.iterrows():
                zone_id = row[label_col]
                rl = row["risk_label"]
                fires = int(row["structural_fires"])
                is_selected = (zone_id == selected)
                icon = "â†’ " if is_selected else ""

                if st.button(
                    f"{icon}{zone_id}  Â·  {rl}  Â·  {fires} fires",
                    key=f"risk_btn_{zone_id}",
                    width="stretch",
                ):
                    # Toggle: click again to deselect
                    st.session_state.selected_zone = zone_id if zone_id != selected else None
                    st.rerun()

            # Clear selection
            if selected is not None:
                if st.button("âœ• Clear selection", key="clear_sel", width="stretch"):
                    st.session_state.selected_zone = None
                    st.rerun()

            st.markdown("---")

            # Detail card for selected zone
            if selected is not None:
                sel_data = active_df[active_df[label_col] == selected]
                if not sel_data.empty:
                    s = sel_data.iloc[0]
                    rl = s["risk_label"].lower()
                    st.markdown(
                        f'<div style="background:#131820;border:1px solid #2A3548;'
                        f'border-radius:8px;padding:12px;margin-bottom:12px">'
                        f'<div style="font-size:16px;font-weight:700">{selected}</div>'
                        f'<span class="risk-{rl}">{s["risk_label"]} Risk '
                        f'({s["risk_score"]:.2f})</span><br>'
                        f'<span style="color:#7B8DA4;font-size:11px">'
                        f'{int(s["structural_fires"])} structural fires Â· '
                        f'{int(s["total_incidents"])} total incidents Â· '
                        f'Fire rate: {s["structural_fire_rate"]:.1%}</span>'
                        f'</div>',
                        unsafe_allow_html=True,
                    )

            # Citywide monthly
            month_cols = [f"month_{m}" for m in range(1, 13)]
            existing = [c for c in month_cols if c in active_df.columns]
            if existing:
                monthly_total = active_df[existing].sum().values
                fig = make_monthly_chart(monthly_total, "Citywide Monthly Pattern")
                st.plotly_chart(fig, use_container_width=True)

    # â”€â”€ TAB: Rankings â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    with tab_rankings:
        ranked = active_df.sort_values("risk_score", ascending=False).reset_index(drop=True)
        ranked.index += 1  # 1-based rank

        display_cols = [
            label_col, "risk_label", "structural_fires",
            "predicted_fires", "total_incidents",
            "structural_fire_rate", "risk_score",
        ]
        display_cols = [c for c in display_cols if c in ranked.columns]

        display_df = ranked[display_cols].copy()
        display_df["structural_fire_rate"] = display_df["structural_fire_rate"].map("{:.1%}".format)
        display_df["risk_score"] = display_df["risk_score"].map("{:.3f}".format)
        display_df["predicted_fires"] = display_df["predicted_fires"].map("{:.0f}".format)
        st.dataframe(display_df, width="stretch", height=500)

        # Borough comparison
        if granularity != "Borough":
            st.markdown("### Borough Comparison")
            fig = make_borough_comparison_chart(boro_features)
            st.plotly_chart(fig, use_container_width=True)

    # â”€â”€ TAB: Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    with tab_model:
        col_m1, col_m2 = st.columns(2)

        with col_m1:
            st.markdown("### Performance Metrics")
            m1, m2, m3 = st.columns(3)
            m1.metric("RÂ² (Train)", f"{results['train']['r2']:.3f}", help="Proportion of variance in fire counts explained by the model. Higher is better; 1.0 is perfect.")
            m2.metric("RMSE", f"{results['train']['rmse']:.1f}", help="Root Mean Squared Error â€” average prediction error in fire counts. Lower is better. Penalizes large errors more heavily than MAE.")
            m3.metric("MAE", f"{results['train']['mae']:.1f}", help="Mean Absolute Error â€” average absolute difference between predicted and actual fire counts. Lower is better.")

            cv = results["cv"]
            st.metric(
                "RÂ² (5-Fold CV)",
                f"{cv['cv_r2_mean']:.3f} Â± {cv['cv_r2_std']:.3f}",
            )

            # Actual vs Predicted
            X, y = data["X"], data["y"]
            preds = results["model"].predict(X)
            fig = make_actual_vs_predicted_chart(y, preds)
            st.plotly_chart(fig, use_container_width=True)

        with col_m2:
            st.markdown("### Feature Importance")
            fig = make_feature_importance_chart(results["importance"])
            st.plotly_chart(fig, use_container_width=True)

            # Risk distribution
            st.markdown("### Risk Distribution")
            fig = make_risk_distribution_chart(active_df["risk_score"].values)
            st.plotly_chart(fig, use_container_width=True)

        # Model comparison table
        if "model_comparison" in data:
            st.markdown("### Model Comparison")
            mc = data["model_comparison"]["comparison"]
            mc_display = mc.copy()
            for col in ["Train RÂ²", "CV RÂ² (mean)", "CV RÂ² (std)", "Train RMSE", "Train MAE"]:
                if col in mc_display.columns:
                    mc_display[col] = mc_display[col].map("{:.3f}".format)
            st.dataframe(mc_display, width="stretch")

            best_name = data["model_comparison"]["best"]["model_name"]
            best_cv = data["model_comparison"]["best"]["cv"]["cv_r2_mean"]
            st.success(f"**Best model: {best_name}** (CV RÂ² = {best_cv:.3f})")

        # Model description
        with st.expander("Model Architecture Details"):
            st.markdown("""
            **Algorithm**: RandomForest Regressor (scikit-learn)
            - 100 decision trees, max depth 12, min 3 samples per leaf
            - Bootstrap sampling with replacement
            - All CPU cores used for parallel training

            **Target Variable**: Count of structural fires (NFIRS codes 100â€“199)
            per geographic unit across the dataset time period.

            **Features** (15 engineered):
            - *Incident counts*: total incidents, structural fires, false alarms
            - *Rate features*: structural fire rate, false alarm rate, medical rate
            - *Response*: average units on scene
            - *Temporal*: year-over-year trend slope, incident volatility (std dev)
            - *Seasonal*: winter concentration (Decâ€“Feb), summer concentration (Junâ€“Aug)
            - *Peak load*: max monthly incidents
            - *Property mix*: residential %, commercial %
            - *Data coverage*: unique years, avg yearly incidents

            **Evaluation**: 5-fold cross-validation with RÂ² scoring.
            Permutation importance computed with 10 repeats.

            **Spatial Levels**:
            - *Zip Code*: Finest granularity (~180 zones). Trained directly.
            - *PUMA*: Neighborhood-level (~55 zones). Features aggregated from zip level.
            - *Borough*: Coarsest (5 zones). Descriptive only.
            """)

    # â”€â”€ TAB: Explorer â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    with tab_explorer:
        st.markdown("### Zone Deep Dive")

        zone_options = active_df[label_col].tolist()
        selected_explorer = st.selectbox(f"Select {granularity}", zone_options)

        if selected_explorer:
            row = active_df[active_df[label_col] == selected_explorer].iloc[0]

            col_e1, col_e2 = st.columns([1, 2])

            with col_e1:
                rl = row["risk_label"].lower()
                st.markdown(
                    f'<div style="text-align:center;padding:20px">'
                    f'<div style="font-size:28px;font-weight:700">{selected_explorer}</div>'
                    f'<div style="margin-top:8px">'
                    f'<span class="risk-{rl}" style="font-size:14px;padding:6px 16px">'
                    f'{row["risk_label"]} Risk ({row["risk_score"]:.2f})</span></div>'
                    f'</div>',
                    unsafe_allow_html=True,
                )

                st.metric("Structural Fires", int(row["structural_fires"]))
                st.metric("Total Incidents", int(row["total_incidents"]))
                st.metric("Structural Fire Rate", f"{row['structural_fire_rate']:.1%}")
                st.metric("Predicted Fires", f"{row['predicted_fires']:.0f}")
                st.metric("Avg Units on Scene", f"{row['avg_units_onscene']:.1f}")

                if "trend_slope" in row.index:
                    trend = row["trend_slope"]
                    st.metric(
                        "Year-over-Year Trend",
                        f"{trend:+.1f} fires/yr",
                        delta=f"{'Increasing' if trend > 0 else 'Decreasing'}",
                        delta_color="inverse",
                    )

            with col_e2:
                # Monthly chart for this zone
                month_cols = [f"month_{m}" for m in range(1, 13)]
                existing = [c for c in month_cols if c in row.index]
                if existing:
                    monthly_vals = [row[c] for c in existing]
                    fig = make_monthly_chart(monthly_vals, f"Monthly Distribution â€” {selected_explorer}")
                    st.plotly_chart(fig, use_container_width=True)

                # Show constituent zips for PUMA view
                if granularity == "PUMA" and "puma_code" in row.index:
                    puma_code = row["puma_code"]
                    constituent_zips = zip_features[zip_features["puma_code"] == puma_code]
                    if not constituent_zips.empty:
                        st.markdown(f"**Constituent Zip Codes** ({len(constituent_zips)})")
                        cz = constituent_zips[["zip_code", "structural_fires", "risk_score", "risk_label"]].sort_values("risk_score", ascending=False).copy()
                        cz["risk_score"] = cz["risk_score"].map("{:.3f}".format)
                        st.dataframe(cz, width="stretch", height=250)

    # â”€â”€ TAB: Buildings â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    with tab_buildings:
        st.markdown("### ğŸ¢ Building-Level Risk Explorer")
        st.caption(
            "Pulls building data from NYC PLUTO (870K+ tax lots) and scores each building "
            "using physical characteristics + neighborhood fire history."
        )

        bldg_col1, bldg_col2 = st.columns([1, 1])
        with bldg_col1:
            bldg_search_mode = st.radio(
                "Search by",
                ["Zip Code", "Borough"],
                horizontal=True,
                key="bldg_search_mode",
            )
        with bldg_col2:
            if bldg_search_mode == "Zip Code":
                available_zips = sorted(zip_features["zip_code"].tolist())
                bldg_zip = st.selectbox("Select Zip Code", available_zips, key="bldg_zip")
                bldg_borough = None
            else:
                bldg_borough = st.selectbox(
                    "Select Borough",
                    ["Manhattan", "Bronx", "Brooklyn", "Queens", "Staten Island"],
                    key="bldg_boro",
                )
                bldg_zip = None

        bldg_limit = st.slider(
            "Max buildings to load",
            min_value=100, max_value=10000, value=2000, step=500,
            help="Higher = more comprehensive but slower to load",
            key="bldg_limit",
        )

        if st.button("ğŸ” Load & Score Buildings", key="load_buildings", width="stretch"):
            with st.spinner("Fetching building data from PLUTO API..."):
                raw_buildings = fetch_pluto_buildings(
                    zip_code=bldg_zip,
                    borough=bldg_borough,
                    limit=bldg_limit,
                )

            if raw_buildings.empty:
                st.warning("No building data returned. Try a different search.")
            else:
                with st.spinner(f"Processing {len(raw_buildings)} buildings..."):
                    buildings = process_pluto_buildings(raw_buildings)

                with st.spinner("Scoring buildings with neighborhood risk..."):
                    scored = score_buildings_with_neighborhood(buildings, zip_features)

                st.session_state["scored_buildings"] = scored
                st.success(f"Scored {len(scored)} buildings.")

        # Display results if we have them
        if "scored_buildings" in st.session_state:
            scored = st.session_state["scored_buildings"]

            # Summary metrics
            bc1, bc2, bc3, bc4 = st.columns(4)
            bc1.metric("Buildings Scored", f"{len(scored):,}")
            bc2.metric("Critical Risk", len(scored[scored["risk_score"] >= 0.75]))
            bc3.metric("Avg Building Age", f"{scored['building_age'].mean():.0f} yrs")
            bc4.metric("Pre-1968 Code", f"{scored['is_pre_code'].mean():.0%}")

            st.markdown("---")

            # Map + Table layout
            bldg_map_col, bldg_info_col = st.columns([3, 2])

            with bldg_map_col:
                st.markdown("**Building Risk Map**")
                try:
                    from streamlit_folium import st_folium
                    import folium
                    from branca.colormap import LinearColormap

                    center_lat = scored["latitude"].mean()
                    center_lng = scored["longitude"].mean()
                    bldg_map = folium.Map(
                        location=[center_lat, center_lng],
                        zoom_start=14,
                        tiles="CartoDB dark_matter",
                    )

                    colormap = LinearColormap(
                        colors=["#2ECC71", "#FF6B35", "#FFAA2B", "#FF3B4E"],
                        vmin=0, vmax=1,
                        caption="Building Fire Risk",
                    )

                    # Sample if too many for performance
                    display_df = scored if len(scored) <= 2000 else scored.sample(2000, random_state=42)

                    for _, bldg in display_df.iterrows():
                        lat = bldg.get("latitude", 0)
                        lng = bldg.get("longitude", 0)
                        if lat == 0 or lng == 0:
                            continue

                        risk = bldg["risk_score"]
                        rc = risk_color(risk)
                        label = bldg["risk_label"]

                        # Visual hierarchy: Critical > High > Moderate > Low
                        if label == "Critical":
                            radius = 10
                            opacity = 0.9
                            border_color = "#FFFFFF"
                            weight = 2
                        elif label == "High":
                            radius = 6
                            opacity = 0.6
                            border_color = rc
                            weight = 1
                        elif label == "Moderate":
                            radius = 4
                            opacity = 0.5
                            border_color = rc
                            weight = 1
                        else:
                            radius = 3
                            opacity = 0.4
                            border_color = rc
                            weight = 1

                        popup_html = (
                            f'<div style="font-family:monospace;font-size:11px;min-width:180px">'
                            f'<b>{bldg.get("address", "N/A")}</b><br>'
                            f'BBL: {bldg.get("bbl", "N/A")}<br>'
                            f'Risk: <span style="color:{rc}">{label} ({risk:.2f})</span><br>'
                            f'Built: {int(bldg["yearbuilt"])} ({int(bldg["building_age"])} yrs)<br>'
                            f'Floors: {int(bldg["numfloors"])} Â· Units: {int(bldg["unitsres"])}<br>'
                            f'Area: {int(bldg["bldgarea"]):,} sqft<br>'
                            f'Class: {bldg.get("bldgclass", "N/A")}'
                            f'</div>'
                        )

                        folium.CircleMarker(
                            location=[lat, lng],
                            radius=radius,
                            color=border_color,
                            weight=weight,
                            fill=True,
                            fill_color=rc,
                            fill_opacity=opacity,
                            popup=folium.Popup(popup_html, max_width=250),
                            tooltip=f'{bldg.get("address", "N/A")} â€” {label}',
                        ).add_to(bldg_map)

                    colormap.add_to(bldg_map)
                    st_folium(bldg_map, width=None, height=450, returned_objects=[])

                except ImportError:
                    st.info("Install `streamlit-folium` for map view.")

            with bldg_info_col:
                st.markdown("**Highest Risk Buildings**")
                top_bldgs = scored.nlargest(15, "risk_score")
                display_bldg_cols = ["address", "risk_label", "risk_score", "yearbuilt",
                                      "numfloors", "unitsres", "bldgclass"]
                display_bldg_cols = [c for c in display_bldg_cols if c in top_bldgs.columns]
                top_display = top_bldgs[display_bldg_cols].copy()
                top_display["risk_score"] = top_display["risk_score"].map("{:.3f}".format)
                top_display["yearbuilt"] = top_display["yearbuilt"].astype(int)
                st.dataframe(top_display, width="stretch", height=350)

                # Risk breakdown
                st.markdown("**Risk Distribution**")
                risk_counts = scored["risk_label"].value_counts()
                for label in ["Critical", "High", "Moderate", "Low"]:
                    count = risk_counts.get(label, 0)
                    pct = count / len(scored) * 100
                    color = {"Critical": "#FF3B4E", "High": "#FFAA2B",
                             "Moderate": "#FF6B35", "Low": "#2ECC71"}[label]
                    st.markdown(
                        f'<div style="display:flex;justify-content:space-between;padding:4px 0">'
                        f'<span style="color:{color};font-weight:600">{label}</span>'
                        f'<span style="color:#7B8DA4">{count:,} ({pct:.1f}%)</span></div>',
                        unsafe_allow_html=True,
                    )

            # Building search
            st.markdown("---")
            st.markdown("**Search Building by Address**")
            addr_search = st.text_input("Enter address (partial match)", key="addr_search")
            if addr_search and len(addr_search) >= 3:
                matches = scored[scored["address"].str.contains(addr_search.upper(), na=False)]
                if matches.empty:
                    st.info("No matches found.")
                else:
                    st.write(f"Found {len(matches)} match(es):")
                    for _, bldg in matches.head(5).iterrows():
                        rl = bldg["risk_label"].lower()
                        st.markdown(
                            f'<div style="background:#131820;border:1px solid #2A3548;'
                            f'border-radius:8px;padding:12px;margin-bottom:8px">'
                            f'<b>{bldg.get("address", "N/A")}</b> '
                            f'<span class="risk-{rl}">{bldg["risk_label"]}</span><br>'
                            f'<span style="color:#7B8DA4;font-size:11px">'
                            f'BBL: {bldg.get("bbl", "N/A")} Â· '
                            f'Built {int(bldg["yearbuilt"])} ({int(bldg["building_age"])} yrs) Â· '
                            f'{int(bldg["numfloors"])} floors Â· '
                            f'{int(bldg["unitsres"])} units Â· '
                            f'{int(bldg["bldgarea"]):,} sqft Â· '
                            f'Class {bldg.get("bldgclass", "N/A")}'
                            f'</span></div>',
                            unsafe_allow_html=True,
                        )

        # â”€â”€ TAB: Validation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    with tab_validation:
        st.markdown("### âœ… Model Validation â€” Temporal Backtest + Ablation")
        st.caption(
            "Train on 2019â€“2022 data, predict 2023+ fire counts. "
            "Also tests whether the model works without the circular structural_fires feature."
        )

        raw_df = data.get("raw_df", None)

        if raw_df is not None and "year" in raw_df.columns and raw_df["year"].nunique() > 2:
            train_years = raw_df[raw_df["year"] <= 2022]
            test_years = raw_df[raw_df["year"] >= 2023]

            if len(test_years) > 0 and len(train_years) > 0:
                from data.feature_engineering import engineer_features_by_zip, get_feature_matrix
                from data.enrichment import enrich_zip_features
                from models.enhanced_model import get_enhanced_feature_matrix, train_tuned_rf, train_xgboost
                from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error

                # Engineer + enrich features for BOTH periods
                # â”€â”€ Layer 1: Data cache (slow â€” API calls, feature engineering) â”€â”€
                data_key = "val_data_cache"
                demo_cache = Path("data/validation_cache.pkl")
                if data_key not in st.session_state:
                    if demo_cache.exists():
                        import pickle
                        with open(demo_cache, "rb") as f:
                            st.session_state[data_key] = pickle.load(f)
                        st.toast("Loaded cached validation data from disk")
                        _vc = st.session_state[data_key]
                        train_features = _vc["train_features"]
                        test_features = _vc["test_features"]
                        X_train_full = _vc["X_train_full"]
                        X_test_full = _vc["X_test_full"]
                        y_train = _vc["y_train"]
                        y_test = _vc["y_test"]
                        fn_full = _vc["fn_full"]
                    else:
                        with st.spinner("Fetching & engineering features (cached after first run)..."):
                            train_features = engineer_features_by_zip(train_years)
                            test_features = engineer_features_by_zip(test_years)
                            try:
                                train_features = enrich_zip_features(train_features)
                                test_features = enrich_zip_features(test_features)
                            except Exception as e:
                                st.warning(f"Enrichment failed for validation: {e}")
                            X_train_full, y_train, fn_full = get_enhanced_feature_matrix(train_features)
                            X_test_full, y_test, _ = get_enhanced_feature_matrix(test_features)
                            st.session_state[data_key] = {
                                "train_features": train_features,
                                "test_features": test_features,
                                "X_train_full": X_train_full,
                                "X_test_full": X_test_full,
                                "y_train": y_train,
                                "y_test": y_test,
                                "fn_full": fn_full,
                            }
                            # Save to disk for future demo mode
                            import pickle
                            with open(demo_cache, "wb") as f:
                                pickle.dump(st.session_state[data_key], f)
                else:
                    _vc = st.session_state[data_key]
                    train_features = _vc["train_features"]
                    test_features = _vc["test_features"]
                    X_train_full = _vc["X_train_full"]
                    X_test_full = _vc["X_test_full"]
                    y_train = _vc["y_train"]
                    y_test = _vc["y_test"]
                    fn_full = _vc["fn_full"]

                if len(X_train_full) > 10 and len(X_test_full) > 5:

                    # â”€â”€ Full model (all features) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                    rf_full = train_tuned_rf(X_train_full, y_train, fn_full)
                    gbm_full = train_xgboost(X_train_full, y_train, fn_full)

                    rf_preds_full = rf_full["model"].predict(X_test_full)
                    gbm_preds_full = gbm_full["model"].predict(X_test_full)

                    rf_oos_r2 = r2_score(y_test, rf_preds_full)
                    gbm_oos_r2 = r2_score(y_test, gbm_preds_full)
                    rf_oos_mae = mean_absolute_error(y_test, rf_preds_full)
                    gbm_oos_mae = mean_absolute_error(y_test, gbm_preds_full)

                    # â”€â”€ Ablation: without structural_fires â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                    # Remove structural_fires from feature set
                    incident_features = {"structural_fires", "total_incidents", "non_structural_fires", "false_alarms", "medical_calls", "structural_fire_rate", "false_alarm_rate", "medical_rate", "avg_units_onscene", "winter_concentration", "summer_concentration", "trend_slope", "incident_volatility", "max_monthly_incidents", "avg_yearly_incidents", "complaints_per_incident"}
                    ablation_cols = [c for c in fn_full if c not in incident_features]
                    ablation_idx = [fn_full.index(c) for c in ablation_cols]

                    X_train_abl = X_train_full[:, ablation_idx]
                    X_test_abl = X_test_full[:, ablation_idx]

                    # â”€â”€ Pick best full model for display â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                    best_full_preds = gbm_preds_full if gbm_oos_r2 > rf_oos_r2 else rf_preds_full
                    best_full_r2 = max(gbm_oos_r2, rf_oos_r2)
                    best_full_mae = gbm_oos_mae if gbm_oos_r2 > rf_oos_r2 else rf_oos_mae
                    best_full_name = gbm_full["model_name"] if gbm_oos_r2 > rf_oos_r2 else rf_full["model_name"]

                    # â”€â”€ Metrics â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                    st.markdown("#### Out-of-Sample Performance")
                    vc1, vc2, vc3, vc4 = st.columns(4)
                    vc1.metric("Train Period", f"{int(train_years['year'].min())}â€“{int(train_years['year'].max())}")
                    vc2.metric("Test Period", f"{int(test_years['year'].min())}â€“{int(test_years['year'].max())}")
                    vc3.metric("RÂ² (Out-of-Sample)", f"{best_full_r2:.3f}", help="R-squared on held-out 2023-2024 data the model never saw during training. Tests whether the model generalizes to new data.")
                    vc4.metric("MAE", f"{best_full_mae:.1f} fires", help="Mean Absolute Error on test data â€” on average, predictions are off by this many fires per zip code.")

                    # â”€â”€ Model Comparison Table â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                    st.markdown("---")
                    st.markdown("#### Model Comparison (Temporal Validation)")

                    comparison_data = [
                        {"Model": "Tuned RF (all features)", "OOS RÂ²": f"{rf_oos_r2:.3f}", "OOS MAE": f"{rf_oos_mae:.1f}", "CV RÂ²": f"{rf_full['cv']['cv_r2_mean']:.3f}", "Features": len(fn_full)},
                        {"Model": "GBM (all features)", "OOS RÂ²": f"{gbm_oos_r2:.3f}", "OOS MAE": f"{gbm_oos_mae:.1f}", "CV RÂ²": f"{gbm_full['cv']['cv_r2_mean']:.3f}", "Features": len(fn_full)},
                    ]
                    st.dataframe(pd.DataFrame(comparison_data), width="stretch")

                    # â”€â”€ Charts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                    st.markdown("---")
                    val_col1, val_col2 = st.columns(2)

                    with val_col1:
                        st.markdown("**Actual vs Predicted (Best Full Model)**")
                        fig = make_actual_vs_predicted_chart(y_test, best_full_preds)
                        fig.update_layout(title=f"Out-of-Sample: {best_full_name}")
                        st.plotly_chart(fig, use_container_width=True)

                    with val_col2:
                        st.markdown("**Risk Tier Validation**")
                        st.caption("Do zip codes flagged as high-risk actually have more fires later?")

                        # Assign risk tiers from training model predictions
                        train_preds_full = rf_full["model"].predict(X_train_full) if rf_oos_r2 >= gbm_oos_r2 else gbm_full["model"].predict(X_train_full)
                        train_risk = np.clip(train_preds_full / (train_preds_full.max() or 1), 0, 1)
                        train_features_copy = train_features.copy()
                        train_features_copy["predicted_risk"] = train_risk

                        def _tier(r):
                            if r >= 0.75: return "Critical"
                            if r >= 0.50: return "High"
                            if r >= 0.25: return "Moderate"
                            return "Low"
                        train_features_copy["risk_tier"] = train_features_copy["predicted_risk"].map(_tier)

                        tier_validation = train_features_copy[["zip_code", "risk_tier"]].merge(
                            test_features[["zip_code", "structural_fires"]],
                            on="zip_code", how="inner"
                        )
                        tier_validation = tier_validation.rename(columns={"structural_fires": "actual_test_fires"})

                        if not tier_validation.empty:
                            tier_summary = tier_validation.groupby("risk_tier")["actual_test_fires"].agg(
                                ["mean", "median", "sum", "count"]
                            ).reindex(["Critical", "High", "Moderate", "Low"])

                            fig2 = go.Figure()
                            colors = {"Critical": "#FF3B4E", "High": "#FFAA2B", "Moderate": "#FF6B35", "Low": "#2ECC71"}
                            for tier in ["Critical", "High", "Moderate", "Low"]:
                                if tier in tier_summary.index:
                                    fig2.add_trace(go.Bar(
                                        x=[tier],
                                        y=[tier_summary.loc[tier, "mean"]],
                                        name=tier,
                                        marker_color=colors[tier],
                                        text=[f"{tier_summary.loc[tier, 'mean']:.1f}"],
                                        textposition="auto",
                                    ))
                            fig2.update_layout(
                                title="Avg Actual Fires (2023+) by Predicted Risk Tier",
                                yaxis_title="Avg Structural Fires per Zip",
                                showlegend=False,
                                height=380,
                                paper_bgcolor="#0B0E11",
                                plot_bgcolor="#131820",
                                font=dict(color="#E8ECF1", family="JetBrains Mono, monospace"),
                            )
                            st.plotly_chart(fig2, use_container_width=True)

                            st.markdown("**Tier Breakdown**")
                            tier_display = tier_summary.copy()
                            tier_display.columns = ["Avg Fires", "Median Fires", "Total Fires", "Zip Count"]
                            tier_display["Avg Fires"] = tier_display["Avg Fires"].map("{:.1f}".format)
                            tier_display["Median Fires"] = tier_display["Median Fires"].map("{:.1f}".format)
                            tier_display["Total Fires"] = tier_display["Total Fires"].astype(int)
                            tier_display["Zip Count"] = tier_display["Zip Count"].astype(int)
                            st.dataframe(tier_display, width="stretch")

                    # â”€â”€ Rolling Temporal Cross-Validation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                    st.markdown("---")
                    st.markdown("#### Rolling Temporal Validation")
                    st.caption(
                        "Train on expanding windows, test on each subsequent year. "
                        "Confirms the model generalizes across different time periods, not just one split."
                    )

                    rolling_cache = Path("data/rolling_cv_cache.pkl")
                    years_available = sorted(raw_df["year"].unique())
                    if rolling_cache.exists():
                        import pickle as _pkl
                        with open(rolling_cache, "rb") as _f:
                            rolling_results = _pkl.load(_f)
                    else:
                     with st.spinner("Running rolling temporal CV..."):
                        rolling_results = []
                        # Need at least 2 years to train, 1 to test
                        for test_yr in years_available[2:]:
                            train_slice = raw_df[raw_df["year"] < test_yr]
                            test_slice = raw_df[raw_df["year"] == test_yr]
                            if len(train_slice) < 50 or len(test_slice) < 10:
                                continue
                            try:
                                tr_feat = engineer_features_by_zip(train_slice)
                                te_feat = engineer_features_by_zip(test_slice)
                                try:
                                    tr_feat = enrich_zip_features(tr_feat)
                                    te_feat = enrich_zip_features(te_feat)
                                except Exception:
                                    pass
                                X_tr, y_tr, fn_r = get_enhanced_feature_matrix(tr_feat)
                                X_te, y_te, _ = get_enhanced_feature_matrix(te_feat)
                                if len(X_tr) < 10 or len(X_te) < 5:
                                    continue
                                from sklearn.ensemble import GradientBoostingRegressor
                                gbm_r = GradientBoostingRegressor(
                                    n_estimators=200, max_depth=5, learning_rate=0.1, random_state=42
                                )
                                gbm_r.fit(X_tr, y_tr)
                                preds_r = gbm_r.predict(X_te)
                                r2_r = r2_score(y_te, preds_r)
                                mae_r = mean_absolute_error(y_te, preds_r)
                                train_yrs = f"{int(train_slice['year'].min())}-{int(train_slice['year'].max())}"
                                rolling_results.append({
                                    "Train Window": train_yrs,
                                    "Test Year": int(test_yr),
                                    "RÂ²": r2_r,
                                    "MAE": mae_r,
                                    "Train Zips": len(X_tr),
                                    "Test Zips": len(X_te),
                                })
                            except Exception:
                                continue

                     if not rolling_cache.exists() and rolling_results:
                        import pickle as _pkl
                        with open(rolling_cache, "wb") as _f:
                            _pkl.dump(rolling_results, _f)

                    if rolling_results:
                        roll_df = pd.DataFrame(rolling_results)
                        rc1, rc2 = st.columns(2)
                        with rc1:
                            fig_roll = go.Figure()
                            fig_roll.add_trace(go.Scatter(
                                x=[str(r["Test Year"]) for r in rolling_results],
                                y=[r["RÂ²"] for r in rolling_results],
                                mode="lines+markers",
                                marker=dict(size=10, color="#FF6B35"),
                                line=dict(color="#FF6B35", width=2),
                                hovertemplate="Test %{x}<br>RÂ² = %{y:.3f}<extra></extra>",
                            ))
                            fig_roll.update_layout(
                                title="RÂ² by Test Year (Rolling Window)",
                                xaxis_title="Test Year",
                                yaxis_title="RÂ² (Out-of-Sample)",
                                height=300,
                                paper_bgcolor="#0B0E11",
                                plot_bgcolor="#131820",
                                font=dict(color="#E8ECF1", family="JetBrains Mono, monospace"),
                                yaxis=dict(range=[0, 1.05]),
                            )
                            st.plotly_chart(fig_roll, use_container_width=True)
                        with rc2:
                            roll_display = roll_df.copy()
                            roll_display["RÂ²"] = roll_display["RÂ²"].map("{:.3f}".format)
                            roll_display["MAE"] = roll_display["MAE"].map("{:.1f}".format)
                            st.dataframe(roll_display, width="stretch", hide_index=True)

                        avg_r2 = np.mean([r["RÂ²"] for r in rolling_results])
                        std_r2 = np.std([r["RÂ²"] for r in rolling_results])
                        st.success(
                            f"**Rolling CV: RÂ² = {avg_r2:.3f} Â± {std_r2:.3f}** across {len(rolling_results)} test years. "
                            f"{'Stable performance confirms the model generalizes well over time.' if std_r2 < 0.1 else 'Some variance across years â€” model performance depends on the time period.'}"
                        )
                    else:
                        st.warning("Not enough yearly data for rolling validation.")

                    # â”€â”€ Borough Spatial Cross-Validation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                    st.markdown("---")
                    st.markdown("#### Borough Holdout Validation")
                    st.caption(
                        "Train on 4 boroughs, test on the held-out borough. "
                        "Tests whether the model generalizes geographically or just memorizes borough-level patterns."
                    )

                    boro_cache = Path("data/borough_cv_cache.pkl")
                    if boro_cache.exists():
                        import pickle as _pkl
                        with open(boro_cache, "rb") as _f:
                            borough_results = _pkl.load(_f)
                    else:
                     boro_cache = Path("data/borough_cv_cache.pkl")
                    if boro_cache.exists():
                        import pickle as _pkl
                        with open(boro_cache, "rb") as _f:
                            borough_results = _pkl.load(_f)
                    else:
                     boro_cache = Path("data/borough_cv_cache.pkl")
                    if boro_cache.exists():
                        import pickle as _pkl
                        with open(boro_cache, "rb") as _f:
                            borough_results = _pkl.load(_f)
                    else:
                     with st.spinner("Running borough holdout CV..."):
                        borough_results = []
                        if "borough" in train_features.columns:
                            borough_col = "borough"
                        elif "zip_code" in train_features.columns:
                            # Map zip codes to boroughs
                            zip_borough = {}
                            bx = range(10451, 10476)
                            mn = list(range(10001, 10041)) + list(range(10101, 10200))
                            bk = list(range(11201, 11257))
                            qn = list(range(11001, 11010)) + list(range(11351, 11698))
                            si = list(range(10301, 10315))
                            for z in bx: zip_borough[str(z)] = "Bronx"
                            for z in mn: zip_borough[str(z)] = "Manhattan"
                            for z in bk: zip_borough[str(z)] = "Brooklyn"
                            for z in qn: zip_borough[str(z)] = "Queens"
                            for z in si: zip_borough[str(z)] = "Staten Island"
                            train_features["_borough"] = train_features["zip_code"].astype(str).map(zip_borough)
                            test_features["_borough"] = test_features["zip_code"].astype(str).map(zip_borough)
                            borough_col = "_borough"
                        else:
                            borough_col = None

                        if borough_col and train_features[borough_col].notna().sum() > 0:
                            all_features_combined = pd.concat([train_features, test_features], ignore_index=True)
                            X_all, y_all, fn_all = get_enhanced_feature_matrix(all_features_combined)

                            for boro in sorted(all_features_combined[borough_col].dropna().unique()):
                                mask = all_features_combined[borough_col] == boro
                                train_mask = ~mask.values
                                test_mask = mask.values

                                if test_mask.sum() < 3 or train_mask.sum() < 10:
                                    continue

                                X_tr_b = X_all[train_mask]
                                y_tr_b = y_all[train_mask]
                                X_te_b = X_all[test_mask]
                                y_te_b = y_all[test_mask]

                                try:
                                    from sklearn.ensemble import GradientBoostingRegressor
                                    gbm_b = GradientBoostingRegressor(
                                        n_estimators=200, max_depth=5, learning_rate=0.1, random_state=42
                                    )
                                    gbm_b.fit(X_tr_b, y_tr_b)
                                    preds_b = gbm_b.predict(X_te_b)
                                    r2_b = r2_score(y_te_b, preds_b)
                                    mae_b = mean_absolute_error(y_te_b, preds_b)
                                    borough_results.append({
                                        "Borough": boro,
                                        "RÂ²": r2_b,
                                        "MAE": mae_b,
                                        "Zips": int(test_mask.sum()),
                                    })
                                except Exception:
                                    continue

                    if borough_results:
                        boro_df = pd.DataFrame(borough_results)
                        bc1, bc2 = st.columns(2)
                        with bc1:
                            colors_boro = {"Manhattan": "#FF3B4E", "Brooklyn": "#FFAA2B", "Queens": "#FF6B35", "Bronx": "#2ECC71", "Staten Island": "#3498DB"}
                            fig_boro = go.Figure()
                            for _, row in boro_df.iterrows():
                                fig_boro.add_trace(go.Bar(
                                    x=[row["Borough"]],
                                    y=[row["RÂ²"]],
                                    name=row["Borough"],
                                    marker_color=colors_boro.get(row["Borough"], "#FF6B35"),
                                    text=[f"{row['RÂ²']:.3f}"],
                                    textposition="auto",
                                ))
                            fig_boro.update_layout(
                                title="RÂ² by Held-Out Borough",
                                yaxis_title="RÂ² (Out-of-Sample)",
                                showlegend=False,
                                height=300,
                                paper_bgcolor="#0B0E11",
                                plot_bgcolor="#131820",
                                font=dict(color="#E8ECF1", family="JetBrains Mono, monospace"),
                                yaxis=dict(range=[min(0, min(r["RÂ²"] for r in borough_results) - 0.1), 1.05]),
                            )
                            st.plotly_chart(fig_boro, use_container_width=True)
                        with bc2:
                            boro_display = boro_df.copy()
                            boro_display["RÂ²"] = boro_display["RÂ²"].map("{:.3f}".format)
                            boro_display["MAE"] = boro_display["MAE"].map("{:.1f}".format)
                            st.dataframe(boro_display, width="stretch", hide_index=True)

                        avg_boro_r2 = np.mean([r["RÂ²"] for r in borough_results])
                        min_boro = min(borough_results, key=lambda x: x["RÂ²"])
                        max_boro = max(borough_results, key=lambda x: x["RÂ²"])
                        st.success(
                            f"**Borough CV: Avg RÂ² = {avg_boro_r2:.3f}** | "
                            f"Best: {max_boro['Borough']} ({max_boro['RÂ²']:.3f}) | "
                            f"Weakest: {min_boro['Borough']} ({min_boro['RÂ²']:.3f}). "
                            f"{'Strong geographic generalization.' if avg_boro_r2 > 0.7 else 'Some boroughs are harder to predict â€” consider borough-specific features.'}"
                        )
                    else:
                        st.warning("Borough information not available for spatial validation.")

                    # â”€â”€ Classification Ablation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                    st.markdown("---")
                    st.markdown("#### Ablation: Can Non-Fire Features Classify Risk?")
                    st.caption(
                        "The full model uses past fire counts â€” partially circular. "
                        "Here we test whether building, demographic, and complaint features "
                        "alone can classify zip codes as High Risk vs Low Risk."
                    )

                    from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score, confusion_matrix as sk_confusion_matrix, f1_score
                    from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier

                    median_fires = np.median(y_train)
                    y_train_cls = (y_train > median_fires).astype(int)
                    y_test_cls = (y_test > median_fires).astype(int)

                    rf_cls = RandomForestClassifier(n_estimators=100, max_depth=12, class_weight="balanced", random_state=42)
                    rf_cls.fit(X_train_abl, y_train_cls)
                    rf_cls_preds = rf_cls.predict(X_test_abl)
                    rf_cls_proba = rf_cls.predict_proba(X_test_abl)[:, 1] if len(rf_cls.classes_) == 2 else None

                    gb_cls = GradientBoostingClassifier(n_estimators=100, max_depth=6, random_state=42)
                    # Note: GBM doesn't support class_weight, so we oversample minority class
                    from sklearn.utils.class_weight import compute_sample_weight
                    gb_sample_weights = compute_sample_weight("balanced", y_train_cls)
                    gb_cls.fit(X_train_abl, y_train_cls, sample_weight=gb_sample_weights)
                    gb_cls_preds = gb_cls.predict(X_test_abl)
                    gb_cls_proba = gb_cls.predict_proba(X_test_abl)[:, 1] if len(gb_cls.classes_) == 2 else None

                    # Pick best model by AUC (not accuracy â€” avoids degenerate all-one-class)
                    rf_auc_cls = roc_auc_score(y_test_cls, rf_cls_proba) if rf_cls_proba is not None else 0
                    gb_auc_cls = roc_auc_score(y_test_cls, gb_cls_proba) if gb_cls_proba is not None else 0

                    if gb_auc_cls >= rf_auc_cls:
                        best_cls, best_proba, best_name_cls = gb_cls, gb_cls_proba, "GBM"
                    else:
                        best_cls, best_proba, best_name_cls = rf_cls, rf_cls_proba, "RandomForest"

                    best_auc = max(rf_auc_cls, gb_auc_cls)

                    # Find optimal threshold using Youden's J statistic
                    from sklearn.metrics import roc_curve
                    fpr, tpr, thresholds = roc_curve(y_test_cls, best_proba)
                    j_scores = tpr - fpr
                    optimal_idx = j_scores.argmax()
                    optimal_threshold = thresholds[optimal_idx]

                    best_preds = (best_proba >= optimal_threshold).astype(int)
                    best_acc = accuracy_score(y_test_cls, best_preds)
                    best_prec = precision_score(y_test_cls, best_preds, zero_division=0)
                    best_rec = recall_score(y_test_cls, best_preds, zero_division=0)
                    best_f1_cls = f1_score(y_test_cls, best_preds, zero_division=0)

                    st.caption(
                        f"Classifying zip codes as **High Risk** (>{int(median_fires)} fires) or "
                        f"**Low Risk** (\u2264{int(median_fires)} fires) using only building, demographic, "
                        f"and complaint features \u2014 zero fire incident data."
                    )

                    mc1, mc2, mc3, mc4, mc5 = st.columns(5)
                    mc1.metric("Accuracy", f"{best_acc:.1%}", help="Percentage of zip codes correctly classified as High Risk or Low Risk using only non-fire features.")
                    if best_auc is not None:
                        mc2.metric("AUC", f"{best_auc:.3f}")
                    mc3.metric("Precision", f"{best_prec:.1%}", help="Of zip codes predicted as High Risk, what percentage actually were. Low precision means many false alarms.")
                    mc4.metric("Recall", f"{best_rec:.1%}", help="Of actual High Risk zip codes, what percentage did the model catch. High recall means few missed dangers.")
                    mc5.metric("Threshold", f"{optimal_threshold:.2f}", help="Optimal probability cutoff (via Youden\'s J statistic) for classifying High vs Low Risk. Values above this threshold are flagged High Risk.")

                    cls_col1, cls_col2 = st.columns(2)

                    with cls_col1:
                        cm = sk_confusion_matrix(y_test_cls, best_preds)
                        labels = ["Low Risk", "High Risk"]
                        fig_cm = go.Figure(data=go.Heatmap(
                            z=cm[::-1],
                            x=labels,
                            y=labels[::-1],
                            text=[[str(v) for v in row] for row in cm[::-1]],
                            texttemplate="%{text}",
                            textfont={"size": 18, "color": "white"},
                            colorscale=[[0, "#131820"], [1, "#FF6B35"]],
                            showscale=False,
                        ))
                        fig_cm.update_layout(
                            title=f"Confusion Matrix ({best_name_cls})",
                            xaxis_title="Predicted",
                            yaxis_title="Actual",
                            height=350,
                            paper_bgcolor="#0B0E11",
                            plot_bgcolor="#131820",
                            font=dict(color="#E8ECF1", family="JetBrains Mono, monospace"),
                        )
                        st.plotly_chart(fig_cm, use_container_width=True)

                    with cls_col2:
                        importances_cls = best_cls.feature_importances_
                        imp_df = pd.DataFrame({
                            "feature": ablation_cols,
                            "importance": importances_cls
                        }).sort_values("importance", ascending=False).head(12)

                        fig_imp_cls = go.Figure(go.Bar(
                            x=imp_df["importance"].values[::-1],
                            y=imp_df["feature"].values[::-1],
                            orientation="h",
                            marker_color="#FF6B35",
                        ))
                        fig_imp_cls.update_layout(
                            title="Top Features for Risk Classification",
                            xaxis_title="Feature Importance (Gini)",
                            height=350,
                            paper_bgcolor="#0B0E11",
                            plot_bgcolor="#131820",
                            font=dict(color="#E8ECF1", family="JetBrains Mono, monospace"),
                            margin=dict(l=180),
                        )
                        st.plotly_chart(fig_imp_cls, use_container_width=True)

                    st.info(
                        f"\U0001f4a1 **Key finding:** Without any fire history, building characteristics and demographics "
                        f"alone correctly classify {best_acc:.0%} of zip codes as high or low risk "
                        f"(AUC = {best_auc:.3f}). This confirms that structural factors \u2014 not just past fires \u2014 "
                        f"drive fire risk."
                    )

                    # â”€â”€ Classification: Rolling Temporal CV â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                    st.markdown("---")
                    st.markdown("##### Rolling Temporal CV (Classification, No Fire History)")
                    st.caption("Same expanding-window approach, but testing the non-fire classification model.")

                    cls_roll_cache = Path("data/cls_rolling_cache.pkl")
                    if cls_roll_cache.exists():
                        import pickle as _pkl
                        with open(cls_roll_cache, "rb") as _f:
                            cls_rolling = _pkl.load(_f)
                    else:
                     cls_roll_cache = Path("data/cls_rolling_cache.pkl")
                    if cls_roll_cache.exists():
                        import pickle as _pkl
                        with open(cls_roll_cache, "rb") as _f:
                            cls_rolling = _pkl.load(_f)
                    else:
                     cls_roll_cache = Path("data/cls_rolling_cache.pkl")
                    if cls_roll_cache.exists():
                        import pickle as _pkl
                        with open(cls_roll_cache, "rb") as _f:
                            cls_rolling = _pkl.load(_f)
                    else:
                     with st.spinner("Running classification rolling CV..."):
                        cls_rolling = []
                        for test_yr in years_available[2:]:
                            train_slice = raw_df[raw_df["year"] < test_yr]
                            test_slice = raw_df[raw_df["year"] == test_yr]
                            if len(train_slice) < 50 or len(test_slice) < 10:
                                continue
                            try:
                                tr_f = engineer_features_by_zip(train_slice)
                                te_f = engineer_features_by_zip(test_slice)
                                try:
                                    tr_f = enrich_zip_features(tr_f)
                                    te_f = enrich_zip_features(te_f)
                                except Exception:
                                    pass
                                X_tr_r, y_tr_r, fn_r = get_enhanced_feature_matrix(tr_f)
                                X_te_r, y_te_r, _ = get_enhanced_feature_matrix(te_f)
                                if len(X_tr_r) < 10 or len(X_te_r) < 5:
                                    continue

                                # Ablation: remove fire features
                                abl_idx_r = [fn_r.index(c) for c in fn_r if c not in incident_features]
                                X_tr_abl_r = X_tr_r[:, abl_idx_r]
                                X_te_abl_r = X_te_r[:, abl_idx_r]

                                med_r = np.median(y_tr_r)
                                y_tr_cls_r = (y_tr_r > med_r).astype(int)
                                y_te_cls_r = (y_te_r > med_r).astype(int)

                                if len(np.unique(y_te_cls_r)) < 2:
                                    continue

                                from sklearn.ensemble import RandomForestClassifier as RFC_r
                                clf_r = RFC_r(n_estimators=100, max_depth=12, class_weight="balanced", random_state=42)
                                clf_r.fit(X_tr_abl_r, y_tr_cls_r)
                                proba_r = clf_r.predict_proba(X_te_abl_r)[:, 1] if len(clf_r.classes_) == 2 else None

                                if proba_r is not None:
                                    auc_r = roc_auc_score(y_te_cls_r, proba_r)
                                    cls_rolling.append({
                                        "Train Window": f"{int(train_slice['year'].min())}-{int(train_slice['year'].max())}",
                                        "Test Year": int(test_yr),
                                        "AUC": auc_r,
                                        "Test Zips": len(X_te_r),
                                    })
                            except Exception:
                                continue

                    if cls_rolling:
                        crc1, crc2 = st.columns(2)
                        with crc1:
                            fig_cr = go.Figure()
                            fig_cr.add_trace(go.Scatter(
                                x=[str(r["Test Year"]) for r in cls_rolling],
                                y=[r["AUC"] for r in cls_rolling],
                                mode="lines+markers",
                                marker=dict(size=10, color="#2ECC71"),
                                line=dict(color="#2ECC71", width=2),
                                hovertemplate="Test %{x}<br>AUC = %{y:.3f}<extra></extra>",
                            ))
                            fig_cr.update_layout(
                                title="Classification AUC by Test Year",
                                xaxis_title="Test Year",
                                yaxis_title="AUC",
                                height=280,
                                paper_bgcolor="#0B0E11",
                                plot_bgcolor="#131820",
                                font=dict(color="#E8ECF1", family="JetBrains Mono, monospace"),
                                yaxis=dict(range=[0.4, 1.05]),
                            )
                            st.plotly_chart(fig_cr, use_container_width=True)
                        with crc2:
                            cr_df = pd.DataFrame(cls_rolling)
                            cr_display = cr_df.copy()
                            cr_display["AUC"] = cr_display["AUC"].map("{:.3f}".format)
                            st.dataframe(cr_display, width="stretch", hide_index=True)

                        avg_cls_auc = np.mean([r["AUC"] for r in cls_rolling])
                        std_cls_auc = np.std([r["AUC"] for r in cls_rolling])
                        st.success(
                            f"**Classification Rolling CV: AUC = {avg_cls_auc:.3f} Â± {std_cls_auc:.3f}** across {len(cls_rolling)} years. "
                            f"{'Non-fire features provide stable risk classification over time.' if std_cls_auc < 0.1 else 'Some variation â€” classification performance depends on the year.'}"
                        )

                    # â”€â”€ Classification: Borough Holdout â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                    st.markdown("##### Borough Holdout (Classification, No Fire History)")
                    st.caption("Hold out one borough at a time. Tests geographic generalization of non-fire risk classification.")

                    cls_boro_cache = Path("data/cls_borough_cache.pkl")
                    if cls_boro_cache.exists():
                        import pickle as _pkl
                        with open(cls_boro_cache, "rb") as _f:
                            cls_borough = _pkl.load(_f)
                    else:
                     cls_boro_cache = Path("data/cls_borough_cache.pkl")
                    if cls_boro_cache.exists():
                        import pickle as _pkl
                        with open(cls_boro_cache, "rb") as _f:
                            cls_borough = _pkl.load(_f)
                    else:
                     cls_boro_cache = Path("data/cls_borough_cache.pkl")
                    if cls_boro_cache.exists():
                        import pickle as _pkl
                        with open(cls_boro_cache, "rb") as _f:
                            cls_borough = _pkl.load(_f)
                    else:
                     with st.spinner("Running classification borough holdout..."):
                        cls_borough = []
                        if borough_col and all_features_combined[borough_col].notna().sum() > 0:
                            abl_cols_all = [c for c in fn_all if c not in incident_features]
                            abl_idx_all = [fn_all.index(c) for c in abl_cols_all]
                            X_all_abl = X_all[:, abl_idx_all]

                            for boro in sorted(all_features_combined[borough_col].dropna().unique()):
                                mask_b = (all_features_combined[borough_col] == boro).values
                                if mask_b.sum() < 3 or (~mask_b).sum() < 10:
                                    continue

                                X_tr_cb = X_all_abl[~mask_b]
                                y_tr_cb = y_all[~mask_b]
                                X_te_cb = X_all_abl[mask_b]
                                y_te_cb = y_all[mask_b]

                                med_cb = np.median(y_tr_cb)
                                y_tr_cls_cb = (y_tr_cb > med_cb).astype(int)
                                y_te_cls_cb = (y_te_cb > med_cb).astype(int)

                                if len(np.unique(y_te_cls_cb)) < 2:
                                    continue

                                try:
                                    from sklearn.ensemble import RandomForestClassifier as RFC_b
                                    clf_b = RFC_b(n_estimators=100, max_depth=12, class_weight="balanced", random_state=42)
                                    clf_b.fit(X_tr_cb, y_tr_cls_cb)
                                    proba_cb = clf_b.predict_proba(X_te_cb)[:, 1] if len(clf_b.classes_) == 2 else None

                                    if proba_cb is not None:
                                        auc_cb = roc_auc_score(y_te_cls_cb, proba_cb)
                                        cls_borough.append({
                                            "Borough": boro,
                                            "AUC": auc_cb,
                                            "Zips": int(mask_b.sum()),
                                        })
                                except Exception:
                                    continue

                    if cls_borough:
                        cbc1, cbc2 = st.columns(2)
                        with cbc1:
                            fig_cb = go.Figure()
                            colors_cb = {"Manhattan": "#FF3B4E", "Brooklyn": "#FFAA2B", "Queens": "#FF6B35", "Bronx": "#2ECC71", "Staten Island": "#3498DB"}
                            for _, row in pd.DataFrame(cls_borough).iterrows():
                                fig_cb.add_trace(go.Bar(
                                    x=[row["Borough"]],
                                    y=[row["AUC"]],
                                    marker_color=colors_cb.get(row["Borough"], "#FF6B35"),
                                    text=[f"{row['AUC']:.3f}"],
                                    textposition="auto",
                                ))
                            fig_cb.update_layout(
                                title="Classification AUC by Held-Out Borough",
                                yaxis_title="AUC",
                                showlegend=False,
                                height=280,
                                paper_bgcolor="#0B0E11",
                                plot_bgcolor="#131820",
                                font=dict(color="#E8ECF1", family="JetBrains Mono, monospace"),
                                yaxis=dict(range=[0.4, 1.05]),
                            )
                            st.plotly_chart(fig_cb, use_container_width=True)
                        with cbc2:
                            cb_df = pd.DataFrame(cls_borough)
                            cb_display = cb_df.copy()
                            cb_display["AUC"] = cb_display["AUC"].map("{:.3f}".format)
                            st.dataframe(cb_display, width="stretch", hide_index=True)

                        avg_cb_auc = np.mean([r["AUC"] for r in cls_borough])
                        min_cb = min(cls_borough, key=lambda x: x["AUC"])
                        max_cb = max(cls_borough, key=lambda x: x["AUC"])
                        st.success(
                            f"**Borough CV: Avg AUC = {avg_cb_auc:.3f}** | "
                            f"Best: {max_cb['Borough']} ({max_cb['AUC']:.3f}) | "
                            f"Weakest: {min_cb['Borough']} ({min_cb['AUC']:.3f}). "
                            f"{'Non-fire features generalize well across boroughs.' if avg_cb_auc > 0.7 else 'Geographic variation â€” some boroughs need different risk indicators.'}"
                        )

                    # â”€â”€ Methodology â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                    with st.expander("Validation Methodology"):
                        st.markdown("""
                        **Temporal Cross-Validation** (strict â€” no data leakage)

                        1. **Training**: All incidents â‰¤2022 â†’ zip-level features â†’ enriched with
                           311 complaints, DOB violations, and PLUTO building data.
                        2. **Testing**: All incidents â‰¥2023 â†’ same feature engineering pipeline.
                        3. **Models**: Both Tuned RandomForest and GradientBoosting are evaluated.
                        4. **Ablation**: Re-trains without the `structural_fires` feature to test
                           whether the model can predict fire risk from building characteristics,
                           complaints, and violations alone (non-circular prediction).

                        **Risk tier validation**: Each zip gets a risk tier from the training model,
                        then we check whether high-risk zips actually had more fires in the test period.
                        Perfect monotonic ordering (Critical > High > Moderate > Low) validates
                        the model's ranking ability.

                        **Why the ablation matters**: Using past fire count to predict future fire
                        count is partially circular. The ablation proves the model captures genuine
                        risk factors (building age, complaints, violations) beyond just "places that
                        had fires will have fires again."
                        """)
                else:
                    st.warning("Not enough data in train or test period for validation.")
            else:
                st.warning("Need data from both â‰¤2022 and â‰¥2023 for temporal validation.")
        else:
            # Cloud mode: load all validation results from pre-computed caches
            val_cache = Path("data/validation_cache.pkl")
            roll_cache = Path("data/rolling_cv_cache.pkl")
            boro_cache = Path("data/borough_cv_cache.pkl")
            cls_roll_cache = Path("data/cls_rolling_cache.pkl")
            cls_boro_cache = Path("data/cls_borough_cache.pkl")

            has_caches = val_cache.exists() and roll_cache.exists()

            if has_caches:
                import pickle as _pkl

                # Load validation data
                with open(val_cache, "rb") as _f:
                    _vc = _pkl.load(_f)
                train_features = _vc["train_features"]
                test_features = _vc["test_features"]
                X_train_full = _vc["X_train_full"]
                X_test_full = _vc["X_test_full"]
                y_train = _vc["y_train"]
                y_test = _vc["y_test"]
                fn_full = _vc["fn_full"]

                if len(X_train_full) > 10 and len(X_test_full) > 5:

                    # â”€â”€ Full model (load from cache if available) â”€â”€
                    if "rf_preds" in _vc:
                        rf_preds_full = _vc["rf_preds"]
                        gbm_preds_full = _vc["gbm_preds"]
                        rf_oos_r2 = _vc["rf_r2"]
                        gbm_oos_r2 = _vc["gbm_r2"]
                        rf_oos_mae = _vc["rf_mae"]
                        gbm_oos_mae = _vc["gbm_mae"]
                        rf_cv = _vc["rf_cv"]
                        gbm_cv = _vc["gbm_cv"]
                        rf_name = _vc.get("rf_name", "RandomForest")
                        gbm_name = _vc.get("gbm_name", "GradientBoosting")
                    else:
                        rf_full = train_tuned_rf(X_train_full, y_train, fn_full)
                        gbm_full = train_xgboost(X_train_full, y_train, fn_full)
                        rf_preds_full = rf_full["model"].predict(X_test_full)
                        gbm_preds_full = gbm_full["model"].predict(X_test_full)
                        rf_oos_r2 = r2_score(y_test, rf_preds_full)
                        gbm_oos_r2 = r2_score(y_test, gbm_preds_full)
                        rf_oos_mae = mean_absolute_error(y_test, rf_preds_full)
                        gbm_oos_mae = mean_absolute_error(y_test, gbm_preds_full)
                        rf_cv = rf_full["cv"]
                        gbm_cv = gbm_full["cv"]
                        rf_name = rf_full["model_name"]
                        gbm_name = gbm_full["model_name"]

                    best_full_preds = gbm_preds_full if gbm_oos_r2 > rf_oos_r2 else rf_preds_full
                    best_full_r2 = max(gbm_oos_r2, rf_oos_r2)
                    best_full_mae = gbm_oos_mae if gbm_oos_r2 > rf_oos_r2 else rf_oos_mae
                    best_full_name = gbm_name if gbm_oos_r2 > rf_oos_r2 else rf_name

                    # â”€â”€ Metrics â”€â”€
                    st.markdown("#### Out-of-Sample Performance")
                    vc1, vc2, vc3, vc4 = st.columns(4)
                    vc1.metric("Train Period", "2019â€“2022")
                    vc2.metric("Test Period", "2023â€“2024")
                    vc3.metric("RÂ² (Out-of-Sample)", f"{best_full_r2:.3f}", help="R-squared on held-out 2023-2024 data the model never saw during training.")
                    vc4.metric("MAE", f"{best_full_mae:.1f} fires", help="Mean Absolute Error on test data.")

                    # â”€â”€ Model Comparison â”€â”€
                    st.markdown("---")
                    st.markdown("#### Model Comparison (Temporal Validation)")
                    comparison_data = [
                        {"Model": "Tuned RF (all features)", "OOS RÂ²": f"{rf_oos_r2:.3f}", "OOS MAE": f"{rf_oos_mae:.1f}", "CV RÂ²": f"{rf_cv['cv_r2_mean']:.3f}", "Features": len(fn_full)},
                        {"Model": "GBM (all features)", "OOS RÂ²": f"{gbm_oos_r2:.3f}", "OOS MAE": f"{gbm_oos_mae:.1f}", "CV RÂ²": f"{gbm_cv['cv_r2_mean']:.3f}", "Features": len(fn_full)},
                    ]
                    st.dataframe(pd.DataFrame(comparison_data), width="stretch")

                    # â”€â”€ Charts â”€â”€
                    st.markdown("---")
                    val_col1, val_col2 = st.columns(2)
                    with val_col1:
                        st.markdown("**Actual vs Predicted (Best Full Model)**")
                        fig = make_actual_vs_predicted_chart(y_test, best_full_preds)
                        fig.update_layout(title=f"Out-of-Sample: {best_full_name}")
                        st.plotly_chart(fig, use_container_width=True)
                    with val_col2:
                        st.markdown("**Risk Tier Validation**")
                        st.caption("Do zip codes flagged as high-risk actually have more fires later?")
                        try:
                            pred_scores = best_full_preds / (best_full_preds.max() + 1e-9)
                            tiers = pd.cut(pred_scores, bins=[-0.01, 0.25, 0.50, 0.75, 1.01], labels=["Low", "Moderate", "High", "Critical"])
                            tier_df = pd.DataFrame({"risk_tier": tiers, "actual_fires": y_test})
                            tier_summary = tier_df.groupby("risk_tier", observed=False).agg(
                                avg_fires=("actual_fires", "mean"),
                                median_fires=("actual_fires", "median"),
                                total_fires=("actual_fires", "sum"),
                                zip_count=("actual_fires", "count"),
                            ).reindex(["Critical", "High", "Moderate", "Low"])
                            tier_colors = {"Critical": "#FF3B4E", "High": "#FFAA2B", "Moderate": "#FF6B35", "Low": "#2ECC71"}
                            fig2 = go.Figure()
                            for tier_name in ["Critical", "High", "Moderate", "Low"]:
                                if tier_name in tier_summary.index:
                                    row = tier_summary.loc[tier_name]
                                    fig2.add_trace(go.Bar(
                                        x=[tier_name], y=[row["avg_fires"]],
                                        marker_color=tier_colors[tier_name],
                                        text=[f"{row['avg_fires']:.1f}"], textposition="auto",
                                        name=tier_name,
                                    ))
                            fig2.update_layout(
                                title="Avg Actual Fires (2023+) by Predicted Risk Tier",
                                yaxis_title="Avg Structural Fires per Zip",
                                showlegend=False, height=350,
                                paper_bgcolor="#0B0E11", plot_bgcolor="#131820",
                                font=dict(color="#E8ECF1", family="JetBrains Mono, monospace"),
                            )
                            st.plotly_chart(fig2, use_container_width=True)
                            st.markdown("**Tier Breakdown**")
                            tier_display = tier_summary.reset_index()
                            st.dataframe(tier_display, width="stretch")
                        except Exception as e:
                            st.warning(f"Tier chart error: {e}")

                    # â”€â”€ Rolling CV from cache â”€â”€
                    if roll_cache.exists():
                        st.markdown("---")
                        st.markdown("#### Rolling Temporal Validation")
                        st.caption("Train on expanding windows, test on each subsequent year.")
                        with open(roll_cache, "rb") as _f:
                            rolling_results = _pkl.load(_f)
                        if rolling_results:
                            rc1, rc2 = st.columns(2)
                            with rc1:
                                fig_roll = go.Figure()
                                fig_roll.add_trace(go.Scatter(
                                    x=[str(r["Test Year"]) for r in rolling_results],
                                    y=[r["RÂ²"] for r in rolling_results],
                                    mode="lines+markers",
                                    marker=dict(size=10, color="#FF6B35"),
                                    line=dict(color="#FF6B35", width=2),
                                ))
                                fig_roll.update_layout(
                                    title="RÂ² by Test Year", xaxis_title="Test Year", yaxis_title="RÂ²",
                                    height=300, paper_bgcolor="#0B0E11", plot_bgcolor="#131820",
                                    font=dict(color="#E8ECF1", family="JetBrains Mono, monospace"),
                                    yaxis=dict(range=[0, 1.05]),
                                )
                                st.plotly_chart(fig_roll, use_container_width=True)
                            with rc2:
                                roll_df = pd.DataFrame(rolling_results)
                                roll_df["RÂ²"] = roll_df["RÂ²"].map("{:.3f}".format)
                                roll_df["MAE"] = roll_df["MAE"].map("{:.1f}".format)
                                st.dataframe(roll_df, width="stretch", hide_index=True)
                            avg_r2 = np.mean([r["RÂ²"] for r in rolling_results])
                            std_r2 = np.std([r["RÂ²"] for r in rolling_results])
                            st.success(f"**Rolling CV: RÂ² = {avg_r2:.3f} Â± {std_r2:.3f}** across {len(rolling_results)} test years. Stable performance confirms generalization.")

                    # â”€â”€ Borough Holdout from cache â”€â”€
                    if boro_cache.exists():
                        st.markdown("---")
                        st.markdown("#### Borough Holdout Validation")
                        st.caption("Train on 4 boroughs, test on the held-out borough.")
                        with open(boro_cache, "rb") as _f:
                            borough_results = _pkl.load(_f)
                        if borough_results:
                            bc1, bc2 = st.columns(2)
                            with bc1:
                                colors_boro = {"Manhattan": "#FF3B4E", "Brooklyn": "#FFAA2B", "Queens": "#FF6B35", "Bronx": "#2ECC71", "Staten Island": "#3498DB"}
                                fig_boro = go.Figure()
                                for r in borough_results:
                                    fig_boro.add_trace(go.Bar(x=[r["Borough"]], y=[r["RÂ²"]], marker_color=colors_boro.get(r["Borough"], "#FF6B35"), text=[f"{r['RÂ²']:.3f}"], textposition="auto"))
                                fig_boro.update_layout(title="RÂ² by Held-Out Borough", yaxis_title="RÂ²", showlegend=False, height=300, paper_bgcolor="#0B0E11", plot_bgcolor="#131820", font=dict(color="#E8ECF1", family="JetBrains Mono, monospace"), yaxis=dict(range=[0, 1.05]))
                                st.plotly_chart(fig_boro, use_container_width=True)
                            with bc2:
                                boro_df = pd.DataFrame(borough_results)
                                boro_df["RÂ²"] = boro_df["RÂ²"].map("{:.3f}".format)
                                boro_df["MAE"] = boro_df["MAE"].map("{:.1f}".format)
                                st.dataframe(boro_df, width="stretch", hide_index=True)
                            avg_br2 = np.mean([r["RÂ²"] for r in borough_results])
                            st.success(f"**Borough CV: Avg RÂ² = {avg_br2:.3f}** â€” strong geographic generalization.")

                    # â”€â”€ Classification Ablation â”€â”€
                    st.markdown("---")
                    st.markdown("#### Ablation: Can Non-Fire Features Classify Risk?")
                    st.caption("Classifying zip codes using only building, demographic, and complaint features.")

                    incident_features = {"structural_fires", "total_incidents", "non_structural_fires", "false_alarms", "medical_calls", "structural_fire_rate", "false_alarm_rate", "medical_rate", "avg_units_onscene", "winter_concentration", "summer_concentration", "trend_slope", "incident_volatility", "max_monthly_incidents", "avg_yearly_incidents", "complaints_per_incident"}
                    ablation_cols = [c for c in fn_full if c not in incident_features]
                    ablation_idx = [fn_full.index(c) for c in ablation_cols]
                    X_train_abl = X_train_full[:, ablation_idx]
                    X_test_abl = X_test_full[:, ablation_idx]

                    median_fires = np.median(y_train)
                    y_train_cls = (y_train > median_fires).astype(int)
                    y_test_cls = (y_test > median_fires).astype(int)
                    st.markdown(f"Classifying zip codes as **High Risk** (>{int(median_fires)} fires) or **Low Risk** (â‰¤{int(median_fires)} fires) using only building, demographic, and complaint features â€” zero fire incident data.")

                    from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
                    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve
                    from sklearn.utils.class_weight import compute_sample_weight

                    rf_cls = RandomForestClassifier(n_estimators=100, max_depth=12, class_weight="balanced", random_state=42)
                    rf_cls.fit(X_train_abl, y_train_cls)
                    rf_cls_proba = rf_cls.predict_proba(X_test_abl)[:, 1] if len(rf_cls.classes_) == 2 else None

                    gb_cls = GradientBoostingClassifier(n_estimators=100, max_depth=6, random_state=42)
                    gb_sample_weights = compute_sample_weight("balanced", y_train_cls)
                    gb_cls.fit(X_train_abl, y_train_cls, sample_weight=gb_sample_weights)
                    gb_cls_proba = gb_cls.predict_proba(X_test_abl)[:, 1] if len(gb_cls.classes_) == 2 else None

                    rf_auc_cls = roc_auc_score(y_test_cls, rf_cls_proba) if rf_cls_proba is not None else 0
                    gb_auc_cls = roc_auc_score(y_test_cls, gb_cls_proba) if gb_cls_proba is not None else 0

                    if gb_auc_cls >= rf_auc_cls:
                        best_cls, best_proba, best_name_cls = gb_cls, gb_cls_proba, "GBM"
                    else:
                        best_cls, best_proba, best_name_cls = rf_cls, rf_cls_proba, "RandomForest"
                    best_auc = max(rf_auc_cls, gb_auc_cls)

                    fpr, tpr, thresholds = roc_curve(y_test_cls, best_proba)
                    j_scores = tpr - fpr
                    optimal_idx = j_scores.argmax()
                    optimal_threshold = thresholds[optimal_idx]
                    best_preds = (best_proba >= optimal_threshold).astype(int)
                    best_acc = accuracy_score(y_test_cls, best_preds)
                    best_prec = precision_score(y_test_cls, best_preds, zero_division=0)
                    best_rec = recall_score(y_test_cls, best_preds, zero_division=0)

                    mc1, mc2, mc3, mc4, mc5 = st.columns(5)
                    mc1.metric("Accuracy", f"{best_acc:.1%}", help="Percentage of zip codes correctly classified.")
                    mc2.metric("AUC", f"{best_auc:.3f}")
                    mc3.metric("Precision", f"{best_prec:.1%}", help="Of predicted High Risk, how many actually were.")
                    mc4.metric("Recall", f"{best_rec:.1%}", help="Of actual High Risk, how many were caught.")
                    mc5.metric("Threshold", f"{optimal_threshold:.2f}", help="Optimal cutoff via Youden's J statistic.")

                    # Confusion matrix
                    cm_col1, cm_col2 = st.columns(2)
                    with cm_col1:
                        st.markdown(f"**Confusion Matrix ({best_name_cls})**")
                        cm = confusion_matrix(y_test_cls, best_preds)
                        fig_cm = go.Figure(data=go.Heatmap(
                            z=cm, x=["Low Risk", "High Risk"], y=["Low Risk", "High Risk"],
                            text=cm, texttemplate="%{text}", colorscale=[[0, "#FF6B35"], [1, "#8B4513"]],
                            showscale=False,
                        ))
                        fig_cm.update_layout(xaxis_title="Predicted", yaxis_title="Actual", height=300, paper_bgcolor="#0B0E11", plot_bgcolor="#131820", font=dict(color="#E8ECF1", family="JetBrains Mono, monospace"))
                        st.plotly_chart(fig_cm, use_container_width=True)
                    with cm_col2:
                        st.markdown("**Top Features for Risk Classification**")
                        importances = best_cls.feature_importances_
                        top_n = min(12, len(ablation_cols))
                        top_idx = np.argsort(importances)[-top_n:]
                        fig_imp = go.Figure(go.Bar(x=importances[top_idx], y=[ablation_cols[i] for i in top_idx], orientation="h", marker_color="#FF6B35"))
                        fig_imp.update_layout(xaxis_title="Feature Importance (Gini)", height=300, paper_bgcolor="#0B0E11", plot_bgcolor="#131820", font=dict(color="#E8ECF1", family="JetBrains Mono, monospace"))
                        st.plotly_chart(fig_imp, use_container_width=True)

                    st.info(f"ğŸ’¡ **Key finding:** Without any fire history, building characteristics and demographics alone correctly classify {best_acc:.0%} of zip codes as high or low risk (AUC = {best_auc:.3f}).")

                    # â”€â”€ Classification Rolling CV from cache â”€â”€
                    if cls_roll_cache.exists():
                        st.markdown("---")
                        st.markdown("##### Rolling Temporal CV (Classification, No Fire History)")
                        with open(cls_roll_cache, "rb") as _f:
                            cls_rolling = _pkl.load(_f)
                        if cls_rolling:
                            crc1, crc2 = st.columns(2)
                            with crc1:
                                fig_cr = go.Figure()
                                fig_cr.add_trace(go.Scatter(x=[str(r["Test Year"]) for r in cls_rolling], y=[r["AUC"] for r in cls_rolling], mode="lines+markers", marker=dict(size=10, color="#2ECC71"), line=dict(color="#2ECC71", width=2)))
                                fig_cr.update_layout(title="Classification AUC by Test Year", xaxis_title="Test Year", yaxis_title="AUC", height=280, paper_bgcolor="#0B0E11", plot_bgcolor="#131820", font=dict(color="#E8ECF1", family="JetBrains Mono, monospace"), yaxis=dict(range=[0.4, 1.05]))
                                st.plotly_chart(fig_cr, use_container_width=True)
                            with crc2:
                                cr_df = pd.DataFrame(cls_rolling)
                                cr_df["AUC"] = cr_df["AUC"].map("{:.3f}".format)
                                st.dataframe(cr_df, width="stretch", hide_index=True)
                            avg_ca = np.mean([r["AUC"] for r in cls_rolling])
                            st.success(f"**Classification Rolling CV: AUC = {avg_ca:.3f}** â€” stable over time.")

                    # â”€â”€ Classification Borough from cache â”€â”€
                    if cls_boro_cache.exists():
                        st.markdown("##### Borough Holdout (Classification, No Fire History)")
                        with open(cls_boro_cache, "rb") as _f:
                            cls_borough = _pkl.load(_f)
                        if cls_borough:
                            cbc1, cbc2 = st.columns(2)
                            with cbc1:
                                colors_cb = {"Manhattan": "#FF3B4E", "Brooklyn": "#FFAA2B", "Queens": "#FF6B35", "Bronx": "#2ECC71", "Staten Island": "#3498DB"}
                                fig_cb = go.Figure()
                                for r in cls_borough:
                                    fig_cb.add_trace(go.Bar(x=[r["Borough"]], y=[r["AUC"]], marker_color=colors_cb.get(r["Borough"], "#FF6B35"), text=[f"{r['AUC']:.3f}"], textposition="auto"))
                                fig_cb.update_layout(title="Classification AUC by Borough", yaxis_title="AUC", showlegend=False, height=280, paper_bgcolor="#0B0E11", plot_bgcolor="#131820", font=dict(color="#E8ECF1", family="JetBrains Mono, monospace"), yaxis=dict(range=[0.4, 1.05]))
                                st.plotly_chart(fig_cb, use_container_width=True)
                            with cbc2:
                                cb_df = pd.DataFrame(cls_borough)
                                cb_df["AUC"] = cb_df["AUC"].map("{:.3f}".format)
                                st.dataframe(cb_df, width="stretch", hide_index=True)
                            avg_cba = np.mean([r["AUC"] for r in cls_borough])
                            st.success(f"**Borough CV: Avg AUC = {avg_cba:.3f}** â€” geographic generalization confirmed.")
            else:
                st.warning("Insufficient data for validation.")

    # â”€â”€â”€ Footer â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    st.divider()
    st.caption(
        "Data: NYC Open Data â€” Incidents Responded to by Fire Companies (NYFIRS) Â· "
        "PLUTO (Primary Land Use Tax Lot Output) Â· "
        "SODA API endpoints tm6d-hbzd, 64uk-42ks Â· "
        "Model: scikit-learn RandomForest Â· "
        "Built with Streamlit"
    )


if __name__ == "__main__":
    main()
