#!/usr/bin/env python3
"""
Fix 1: Remove QuantileTransformer (causes forced 25/25/25/25 split)
Fix 2: Add temporal validation (train on 2018-2022, test on 2023+)
Fix 3: Fix postcode->zipcode in scoring merge

Run from: /Users/jonathanepstein/Documents/Personal/nyc_fire_app
Usage: python3 fix_scoring.py
"""
import os
os.chdir("/Users/jonathanepstein/Documents/Personal/nyc_fire_app")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# FIX 1: building_data.py â€” Fix scoring distribution
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
content = open("data/building_data.py").read()

# Remove QuantileTransformer and use natural distribution with threshold tuning
old_scoring = '''    # Composite risk score
    df["risk_score"] = (
        0.40 * df["neighborhood_risk"] +
        0.35 * df["building_vulnerability"] +
        0.25 * df["neighborhood_fire_rate"].clip(0, 1) * 3  # Scale up fire rate
    ).clip(0, 1)

    # Normalize to 0â€“1 range using quantiles for better spread
    if len(df) > 10:
        from sklearn.preprocessing import QuantileTransformer
        qt = QuantileTransformer(output_distribution="uniform", random_state=42)
        df["risk_score"] = qt.fit_transform(df[["risk_score"]]).flatten()

    # Risk labels
    def _label(r):
        if r >= 0.75: return "Critical"
        if r >= 0.50: return "High"
        if r >= 0.25: return "Moderate"
        return "Low"

    df["risk_label"] = df["risk_score"].map(_label)'''

new_scoring = '''    # Composite risk score (raw)
    raw_score = (
        0.40 * df["neighborhood_risk"] +
        0.35 * df["building_vulnerability"] +
        0.25 * df["neighborhood_fire_rate"].clip(0, 1) * 3
    ).clip(0, 1)

    # Apply nonlinear transformation to create realistic skewed distribution
    # Most buildings should be low/moderate risk, few should be critical
    # Using a power transform: raises the bar for high scores
    df["risk_score"] = np.power(raw_score, 0.7)  # Compress toward lower end
    
    # Min-max normalize within the batch to use full 0-1 range
    rmin = df["risk_score"].min()
    rmax = df["risk_score"].max()
    if rmax > rmin:
        df["risk_score"] = (df["risk_score"] - rmin) / (rmax - rmin)
    
    # Risk labels with skewed thresholds (fewer critical, more low)
    def _label(r):
        if r >= 0.85: return "Critical"
        if r >= 0.65: return "High"
        if r >= 0.35: return "Moderate"
        return "Low"

    df["risk_label"] = df["risk_score"].map(_label)'''

if old_scoring in content:
    content = content.replace(old_scoring, new_scoring)
    print("[OK] building_data.py: Removed QuantileTransformer, added skewed scoring")
else:
    print("[SKIP] building_data.py: Old scoring block not found")

# Fix postcode -> zipcode in the merge
content = content.replace(
    'if "postcode" in df.columns and "zip_code" in zip_features_df.columns:',
    'if "zipcode" in df.columns and "zip_code" in zip_features_df.columns:'
)
content = content.replace(
    'df["postcode"] = df["postcode"].astype(str)',
    'df["zipcode"] = df["zipcode"].astype(str)'
)
content = content.replace(
    'df = df.merge(zip_risk, left_on="postcode", right_on="zip_code", how="left")',
    'df = df.merge(zip_risk, left_on="zipcode", right_on="zip_code", how="left")'
)

open("data/building_data.py", "w").write(content)
print("[OK] building_data.py: Fixed postcode->zipcode in merge")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# FIX 2: app.py â€” Add validation tab + fix building risk_color import
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
content = open("app.py").read()

# Add validation tab to tabs list
old_tabs = '''    tab_map, tab_rankings, tab_model, tab_explorer, tab_buildings = st.tabs(
        ["ðŸ—ºï¸ Risk Map", "ðŸ“Š Rankings", "ðŸ§  Model", "ðŸ” Explorer", "ðŸ¢ Buildings"]
    )'''

new_tabs = '''    tab_map, tab_rankings, tab_model, tab_explorer, tab_buildings, tab_validation = st.tabs(
        ["ðŸ—ºï¸ Risk Map", "ðŸ“Š Rankings", "ðŸ§  Model", "ðŸ” Explorer", "ðŸ¢ Buildings", "âœ… Validation"]
    )'''

if old_tabs in content:
    content = content.replace(old_tabs, new_tabs)
    print("[OK] app.py: Added validation tab")
else:
    print("[SKIP] app.py: Tabs definition not found")

# Insert validation tab content before the footer
old_footer = '''    # â”€â”€â”€ Footer â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    st.divider()
    st.caption(
        "Data: NYC Open Data'''

validation_tab = '''    # â”€â”€ TAB: Validation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    with tab_validation:
        st.markdown("### âœ… Model Validation â€” Temporal Backtest")
        st.caption(
            "Train the model on 2018â€“2022 data, predict 2023+ fire counts per zip code, "
            "then compare predictions against actual 2023+ outcomes."
        )

        raw_df = data["raw_df"]

        if "year" in raw_df.columns and raw_df["year"].nunique() > 2:
            # Split: train on <=2022, test on >=2023
            train_years = raw_df[raw_df["year"] <= 2022]
            test_years = raw_df[raw_df["year"] >= 2023]

            if len(test_years) > 0 and len(train_years) > 0:
                from data.feature_engineering import engineer_features_by_zip, get_feature_matrix
                from models.fire_model import FireRiskModel

                # Engineer features on train period
                train_features = engineer_features_by_zip(train_years)
                X_train, y_train, fn = get_feature_matrix(train_features)

                # Engineer features on test period (for actual counts)
                test_features = engineer_features_by_zip(test_years)

                if len(X_train) > 10 and len(test_features) > 5:
                    # Train temporal model
                    temp_model = FireRiskModel(n_estimators=100, max_depth=12)
                    temp_results = temp_model.fit(X_train, y_train, fn)

                    # Predict on test features
                    X_test, y_test, _ = get_feature_matrix(test_features)
                    test_preds = temp_model.predict(X_test)

                    # Merge for comparison
                    comparison = test_features[["zip_code", "structural_fires"]].copy()
                    comparison["predicted"] = test_preds
                    comparison["actual"] = comparison["structural_fires"]
                    comparison["error"] = comparison["predicted"] - comparison["actual"]
                    comparison["abs_error"] = comparison["error"].abs()

                    # Assign risk tiers from training model
                    _, train_risk = temp_model.predict_with_risk(X_train)
                    train_features["predicted_risk"] = train_risk

                    def _tier(r):
                        if r >= 0.75: return "Critical"
                        if r >= 0.50: return "High"
                        if r >= 0.25: return "Moderate"
                        return "Low"
                    train_features["risk_tier"] = train_features["predicted_risk"].map(_tier)

                    # Metrics
                    from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
                    r2 = r2_score(y_test, test_preds)
                    mae = mean_absolute_error(y_test, test_preds)
                    rmse = np.sqrt(mean_squared_error(y_test, test_preds))

                    vc1, vc2, vc3, vc4 = st.columns(4)
                    vc1.metric("Train Period", f"{int(train_years['year'].min())}â€“{int(train_years['year'].max())}")
                    vc2.metric("Test Period", f"{int(test_years['year'].min())}â€“{int(test_years['year'].max())}")
                    vc3.metric("RÂ² (Out-of-Sample)", f"{r2:.3f}")
                    vc4.metric("MAE", f"{mae:.1f} fires")

                    st.markdown("---")

                    val_col1, val_col2 = st.columns(2)

                    with val_col1:
                        st.markdown("**Actual vs Predicted (Test Period)**")
                        fig = make_actual_vs_predicted_chart(y_test, test_preds)
                        fig.update_layout(title="Out-of-Sample: Actual vs Predicted (2023+)")
                        st.plotly_chart(fig, use_container_width=True)

                    with val_col2:
                        st.markdown("**Risk Tier Validation**")
                        st.caption("Do zip codes flagged as high-risk actually have more fires later?")

                        # Join train risk tiers with test actual fires
                        tier_validation = train_features[["zip_code", "risk_tier"]].merge(
                            test_features[["zip_code", "structural_fires"]],
                            on="zip_code", how="inner"
                        )
                        tier_validation = tier_validation.rename(columns={"structural_fires": "actual_test_fires"})

                        if not tier_validation.empty:
                            tier_summary = tier_validation.groupby("risk_tier")["actual_test_fires"].agg(
                                ["mean", "median", "sum", "count"]
                            ).reindex(["Critical", "High", "Moderate", "Low"])

                            import plotly.graph_objects as go
                            fig2 = go.Figure()
                            colors = {"Critical": "#FF3B4E", "High": "#FFAA2B", "Moderate": "#FF6B35", "Low": "#2ECC71"}
                            for tier in ["Critical", "High", "Moderate", "Low"]:
                                if tier in tier_summary.index:
                                    fig2.add_trace(go.Bar(
                                        x=[tier],
                                        y=[tier_summary.loc[tier, "mean"]],
                                        name=tier,
                                        marker_color=colors[tier],
                                        text=[f"{tier_summary.loc[tier, 'mean']:.1f}"],
                                        textposition="auto",
                                    ))
                            fig2.update_layout(
                                title="Avg Actual Fires (2023+) by Predicted Risk Tier (2018â€“2022)",
                                yaxis_title="Avg Structural Fires per Zip",
                                showlegend=False,
                                height=380,
                                paper_bgcolor="#0B0E11",
                                plot_bgcolor="#131820",
                                font=dict(color="#E8ECF1", family="JetBrains Mono, monospace"),
                            )
                            st.plotly_chart(fig2, use_container_width=True)

                            st.markdown("**Tier Breakdown**")
                            tier_display = tier_summary.copy()
                            tier_display.columns = ["Avg Fires", "Median Fires", "Total Fires", "Zip Count"]
                            tier_display["Avg Fires"] = tier_display["Avg Fires"].map("{:.1f}".format)
                            tier_display["Median Fires"] = tier_display["Median Fires"].map("{:.1f}".format)
                            tier_display["Total Fires"] = tier_display["Total Fires"].astype(int)
                            tier_display["Zip Count"] = tier_display["Zip Count"].astype(int)
                            st.dataframe(tier_display, use_container_width=True)

                    # Top errors
                    st.markdown("---")
                    st.markdown("**Largest Prediction Errors**")
                    worst = comparison.nlargest(10, "abs_error")[["zip_code", "actual", "predicted", "error"]].copy()
                    worst["predicted"] = worst["predicted"].map("{:.1f}".format)
                    worst["error"] = worst["error"].map("{:+.1f}".format)
                    st.dataframe(worst, use_container_width=True)

                    with st.expander("Validation Methodology"):
                        st.markdown("""
                        **Temporal Cross-Validation**

                        This is a strict temporal backtest â€” no data leakage:

                        1. **Training data**: All fire incidents from 2018â€“2022. Features are
                           engineered at the zip-code level (structural fire counts, rates,
                           seasonal patterns, trends, etc.)

                        2. **Model**: Same RandomForest architecture (100 trees, depth 12)
                           trained *only* on 2018â€“2022 features.

                        3. **Test data**: Actual 2023+ fire incidents. We engineer the same
                           features and compare predicted vs actual structural fire counts.

                        4. **Risk tier validation**: We assign each zip a risk tier based on
                           the 2018â€“2022 model, then check whether "Critical" zips actually
                           had more fires in 2023+. If the model is good, Critical > High >
                           Moderate > Low.

                        **Key metric**: The RÂ² score here is *out-of-sample* â€” it tells you
                        how well the model generalizes to future data it's never seen.
                        An RÂ² > 0.5 is good, > 0.7 is strong.

                        **Building-level note**: This validation operates at the zip level
                        because NYFIRS data doesn't include building addresses. The building
                        risk scores inherit neighborhood risk as their largest component (40%),
                        so zip-level validation also validates the building scores indirectly.
                        """)
                else:
                    st.warning("Not enough data in train or test period for validation.")
            else:
                st.warning("Need data from both â‰¤2022 and â‰¥2023 for temporal validation. "
                           "Try increasing the data fetch limit or retraining.")
        else:
            st.warning("Insufficient temporal data for validation. "
                       "The model needs multi-year incident data. Try retraining with more data.")

    # â”€â”€â”€ Footer â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    st.divider()
    st.caption(
        "Data: NYC Open Data'''

if old_footer in content:
    content = content.replace(old_footer, validation_tab)
    print("[OK] app.py: Added validation tab content")
else:
    print("[SKIP] app.py: Footer block not found")

open("app.py", "w").write(content)

# Clear cache
import shutil
if os.path.exists(".cache"):
    shutil.rmtree(".cache")
    print("[OK] Cleared .cache")

print("\n=== All fixes applied. Run: streamlit run app.py ===")
